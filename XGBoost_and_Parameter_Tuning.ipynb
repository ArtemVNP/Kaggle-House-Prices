{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb  #GBM algorithm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   # Perforing grid search\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/new_train.csv')\n",
    "test_data = pd.read_csv('data/new_test.csv')\n",
    "\n",
    "print train_data.shape\n",
    "display(train_data.head(1))\n",
    "# display(train_data.info())\n",
    "\n",
    "print test_data.shape\n",
    "display(test_data.head(1))\n",
    "# display(test_data.info())\n",
    "train_length = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def common_num_range(start,stop,step):\n",
    "    \n",
    "    startlen = stoplen = steplen = 0\n",
    "    if '.' in str(start):\n",
    "        startlen = len(str(start)) - str(start).index('.') - 1\n",
    "    if '.' in str(stop):\n",
    "        stoplen = len(str(stop)) - str(stop).index('.') - 1\n",
    "    if '.' in str(step):\n",
    "        steplen = len(str(step)) - str(step).index('.') - 1\n",
    "    \n",
    "    maxlen = startlen\n",
    "    if stoplen > maxlen:\n",
    "        maxlen = stoplen\n",
    "    if steplen > maxlen:\n",
    "        maxlen = steplen\n",
    "    \n",
    "    power = math.pow(10, maxlen)\n",
    "    \n",
    "    if startlen == 0 and stoplen == 0 and steplen == 0:\n",
    "        return range(start, stop, step)\n",
    "    else:\n",
    "        return [num / power for num in range(int(start*power), int(stop*power), int(step*power))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id = train_data['Id']\n",
    "train_Y = train_data['SalePrice']\n",
    "train_data.drop(['Id', 'SalePrice'], axis=1, inplace=True)\n",
    "train_X = train_data\n",
    "\n",
    "test_Id = test_data['Id']\n",
    "test_data.drop('Id', axis=1, inplace=True)\n",
    "test_X = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# formatting for xgb\n",
    "dtrain = xgb.DMatrix(train_X, label=train_Y)\n",
    "dtest = xgb.DMatrix(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost  & Parameter Tuning\n",
    "\n",
    "Ref: [Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parameters Tuning Plan\n",
    "\n",
    "The overall parameters can be divided into 3 categories:\n",
    "\n",
    "1. General Parameters: Guide the overall functioning\n",
    "2. Booster Parameters: Guide the individual booster (tree/regression) at each step\n",
    "3. Learning Task Parameters: Guide the optimization performed\n",
    "\n",
    "In `XGBRegressor`:\n",
    "```\n",
    "class xgboost.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', nthread=-1, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, seed=0, missing=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cross_validate(xgb_regressor, cv_paramters, dtrain, \n",
    "              cv_folds = 5, early_stopping_rounds = 50, perform_progress=False):\n",
    "    \"\"\"\n",
    "    xgb model cross validate to choose best param from giving cv_paramters.\n",
    "    \n",
    "    @param cv_paramters:dict,where to choose best param. {'param':[1,2,3]}\n",
    "    @param dtrain:xgboost.DMatrix, training data formatted for xgb\n",
    "    @param early_stopping_rounds: Activates early stopping.Stop when perfomance \n",
    "                                  does not improve for some rounds\n",
    "    \"\"\"\n",
    "    # get initial parameters\n",
    "    xgb_param = xgb_regressor.get_xgb_params()\n",
    "    \n",
    "    # save best param\n",
    "    best_param = {}\n",
    "    best_cvresult = None\n",
    "    min_mean_rmse = float(\"inf\")\n",
    "    \n",
    "    for param, values in cv_paramters.items():\n",
    "        print '===========Tuning paramter:',param,'==========='\n",
    "        for value in values:\n",
    "            # set the param's value\n",
    "            xgb_param[param] = value\n",
    "            \n",
    "            # cv to tune param from values\n",
    "            cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=xgb_param['n_estimators'], \n",
    "                              nfold=cv_folds, metrics='rmse', \n",
    "                              early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "            # calcuate the mean of several final rmses\n",
    "            round_count = cvresult.shape[0]\n",
    "            mean_rmse = cvresult.loc[round_count-11:round_count-1,'test-rmse-mean'].mean()\n",
    "            \n",
    "            if perform_progress:\n",
    "                std_rmse = cvresult.loc[round_count-11:round_count-1,'test-rmse-std'].mean()\n",
    "\n",
    "                if isinstance(value, int):\n",
    "                    print \"%s=%d CV RMSE : Mean = %.7g | Std = %.7g\" % (param, value, mean_rmse, std_rmse)\n",
    "                else:\n",
    "                    print \"%s=%f CV RMSE : Mean = %.7g | Std = %.7g\" % (param, value, mean_rmse, std_rmse)\n",
    "\n",
    "            if mean_rmse < min_mean_rmse:\n",
    "                best_param[param] = value\n",
    "                best_cvresult = cvresult\n",
    "                min_mean_rmse = mean_rmse\n",
    "        \n",
    "        print \"best \", param, \" = \", best_param[param]\n",
    "    \n",
    "    return best_param, min_mean_rmse, best_cvresult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fit(regressor, train_x, train_y, performCV=True, \n",
    "              printFeatureImportance=True, cv_folds=5):\n",
    "    \n",
    "    # fir the train data\n",
    "    regressor.fit(train_x, train_y)\n",
    "    \n",
    "    # Predict training set\n",
    "    train_predictions = regressor.predict(train_x)\n",
    "    mse = metrics.mean_absolute_error(train_y, regressor.predict(train_x))\n",
    "    print \"Model training report:\"\n",
    "    print(\"MSE: %.9f\" % mse)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    if performCV:\n",
    "        cv_score = cross_validation.cross_val_score(regressor, train_x, train_y, \n",
    "                                               cv=cv_folds, scoring='r2')\n",
    "        print \"CV Score : Mean = %.7g | Std = %.7g | Min = %.7g | Max = %.7g\" % \\\n",
    "                (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score))\n",
    "    \n",
    "    # Print Feature Importance\n",
    "    if printFeatureImportance:\n",
    "        feature_importances = pd.Series(regressor.feature_importances_, train_x.columns.values)\n",
    "        feature_importances = feature_importances.sort_values(ascending=False)\n",
    "        feature_importances= feature_importances.head(40)\n",
    "        feature_importances.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "    \n",
    "    return regressor, feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_regressor = XGBRegressor(seed=10)\n",
    "xgb_regressor, feature_importances = model_fit(xgb_regressor,train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Choose a relatively high learning_rate,optimum n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_test1 = {'n_estimators':range(100,210,10)}\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate =0.02,\n",
    "    \n",
    "                max_depth=5,\n",
    "                min_child_weight=1,\n",
    "                gamma=0,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_lambda = 0.1,\n",
    "                reg_alpha = 0.1,\n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "best_param, min_mean_rmse, best_cvresult = \\\n",
    "                model_cross_validate(xgb_regressor, param_test1, dtrain, perform_progress=True)\n",
    "\n",
    "print 'best params:', best_param\n",
    "print 'min_mean_rmse:', min_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.02,\n",
    "                n_estimators=200,\n",
    "    \n",
    "                max_depth=5,\n",
    "                min_child_weight=1,\n",
    "                gamma=0,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_lambda = 0.1,\n",
    "                reg_alpha = 0.1,\n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "xgb_regressor, feature_importances = model_fit(xgb_regressor,train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.Fix learning rate and number of estimators for tuning tree-based parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tune `max_depth` and `min_child_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_test2 = {'max_depth':range(1,6,1),\n",
    "               'min_child_weight':range(1,8,1)}\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.02,\n",
    "                n_estimators=200,\n",
    "    \n",
    "                gamma=0,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_lambda = 0.1,\n",
    "                reg_alpha = 0.1,\n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "best_param, min_mean_rmse, best_cvresult = \\\n",
    "                model_cross_validate(xgb_regressor, param_test2, dtrain, perform_progress=True)\n",
    "\n",
    "print 'best params:', best_param\n",
    "print 'min_mean_rmse:', min_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.2,\n",
    "                n_estimators=200,\n",
    "                max_depth=4,\n",
    "                min_child_weight=5,\n",
    "    \n",
    "                gamma=0,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_lambda = 0.1,\n",
    "                reg_alpha = 0.1,\n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "xgb_regressor, feature_importances = model_fit(xgb_regressor,train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tune `gamma`,Minimum loss reduction required to make a further partition on a leaf node of the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_test2 = {'gamma':[0, 0.1, 0.01, 0.001,0.0001]}\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.2,\n",
    "                n_estimators=200,\n",
    "                max_depth=4,\n",
    "                min_child_weight=5,\n",
    "    \n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_lambda = 0.1,\n",
    "                reg_alpha = 0.1,\n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "best_param, min_mean_rmse, best_cvresult = \\\n",
    "                model_cross_validate(xgb_regressor, param_test2, dtrain, perform_progress=True)\n",
    "\n",
    "print 'best params:', best_param\n",
    "print 'min_mean_rmse:', min_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.1,\n",
    "                n_estimators=190,\n",
    "                max_depth=3,\n",
    "                min_child_weight=2,\n",
    "                gamma=0.001,\n",
    "                \n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_lambda = 0.1,\n",
    "                reg_alpha = 0.1,\n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "xgb_regressor, feature_importances = model_fit(xgb_regressor,train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tune `subsample` and `colsample_bytree`\n",
    "\n",
    "- subsample : Subsample ratio of the training instance.\n",
    "- colsample_bytree : Subsample ratio of columns when constructing each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "param_test2 = {'subsample':common_num_range(0.6, 0.9, 0.01),\n",
    "               'colsample_bytree':common_num_range(0.6, 0.9, 0.01)}\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.1,\n",
    "                n_estimators=190,\n",
    "                max_depth=3,\n",
    "                min_child_weight=2,\n",
    "                gamma=0.001,\n",
    "                \n",
    "                reg_lambda = 0.1,\n",
    "                reg_alpha = 0.1,\n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "best_param, min_mean_rmse, best_cvresult = \\\n",
    "                model_cross_validate(xgb_regressor, param_test2, dtrain, perform_progress=True)\n",
    "\n",
    "print 'best params:', best_param\n",
    "print 'min_mean_rmse:', min_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.1,\n",
    "                n_estimators=190,\n",
    "                max_depth=3,\n",
    "                min_child_weight=2,\n",
    "                gamma=0.001,\n",
    "                subsample=0.71,\n",
    "                colsample_bytree=0.66,\n",
    "    \n",
    "                reg_lambda = 0.1,\n",
    "                reg_alpha = 0.1,\n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "xgb_regressor, feature_importances = model_fit(xgb_regressor,train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_test2 = {'reg_lambda':common_num_range(0, 0.5, 0.01),\n",
    "               'reg_alpha':common_num_range(0, 0.5, 0.01)}\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.1,\n",
    "                n_estimators=190,\n",
    "                max_depth=3,\n",
    "                min_child_weight=2,\n",
    "                gamma=0.001,\n",
    "                subsample=0.71,\n",
    "                colsample_bytree=0.66,\n",
    "    \n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "best_param, min_mean_rmse, best_cvresult = \\\n",
    "                model_cross_validate(xgb_regressor, param_test2, dtrain, perform_progress=True)\n",
    "\n",
    "print 'best params:', best_param\n",
    "print 'min_mean_rmse:', min_mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.1,\n",
    "                n_estimators=190,\n",
    "                max_depth=3,\n",
    "                min_child_weight=2,\n",
    "                gamma=0.001,\n",
    "                subsample=0.71,\n",
    "                colsample_bytree=0.66,\n",
    "                reg_lambda = 0.32,\n",
    "                reg_alpha = 0.32,\n",
    "    \n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "xgb_regressor, feature_importances = model_fit(xgb_regressor,train_X, train_Y)                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramters Micro-tuning\n",
    "\n",
    "```\n",
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.1,\n",
    "                n_estimators=190,\n",
    "                max_depth=3,\n",
    "                min_child_weight=2,\n",
    "                gamma=0.001,\n",
    "                subsample=0.71,\n",
    "                colsample_bytree=0.66,\n",
    "                reg_lambda = 0.32,\n",
    "                reg_alpha = 0.32,\n",
    "    \n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.01,\n",
    "                n_estimators=3000,\n",
    "                max_depth=3,\n",
    "                min_child_weight=2,\n",
    "                gamma=0.001,\n",
    "                subsample=0.71,\n",
    "                colsample_bytree=0.66,\n",
    "                reg_lambda = 0.32,\n",
    "                reg_alpha = 0.32,\n",
    "    \n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "xgb_regressor, feature_importances = model_fit(xgb_regressor,train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_regressor = XGBRegressor(\n",
    "                learning_rate=0.01,\n",
    "                n_estimators=3000,\n",
    "                max_depth=3,\n",
    "                min_child_weight=2,\n",
    "                gamma=0.001,\n",
    "                subsample=0.72,\n",
    "                colsample_bytree=0.63,\n",
    "                reg_lambda = 0.29,\n",
    "                reg_alpha = 0.34,\n",
    "    \n",
    "                scale_pos_weight=1,\n",
    "                objective= 'reg:linear',\n",
    "                seed=10)\n",
    "\n",
    "xgb_regressor, feature_importances = model_fit(xgb_regressor,train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_predictions = xgb_regressor.predict(test_X)\n",
    "xgb_predictions = np.power(np.e, xgb_predictions) - 1\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"Id\": test_Id,\n",
    "        \"SalePrice\": xgb_predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"result/xgb_param_tune_predictions_2_11.csv\", index=False)\n",
    "\n",
    "print \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor  #GBM algorithm\n",
    "\n",
    "gbm_regressor = GradientBoostingRegressor(\n",
    "                learning_rate=0.09,\n",
    "                n_estimators=180,\n",
    "                max_depth=3, \n",
    "                max_features = 60,\n",
    "                min_samples_leaf=1,\n",
    "                min_samples_split=27,\n",
    "                subsample = 0.82,\n",
    "                random_state=10,\n",
    "                verbose=0)\n",
    "gbm_regressor, feature_importances = model_fit(gbm_regressor, train_X, train_Y)\n",
    "\n",
    "gbm_predictions = gbm_regressor.predict(test_X)\n",
    "gbm_predictions = np.power(np.e, gbm_predictions) - 1\n",
    "\n",
    "predictions = (gbm_predictions + xgb_predictions) / 2\n",
    "submission = pd.DataFrame({\n",
    "        \"Id\": test_Id,\n",
    "        \"SalePrice\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"result/xgb_gbm_param_tune_predictions_2_11.csv\", index=False)\n",
    "\n",
    "print \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
